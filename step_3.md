# Acc√©l√©rons l'entrainement avec de l'IAGen

[<img src="img/Indiana-Jones-Temple-of-Doom.png"  alt="Indiana Jones Temple of Doom">](https://www.youtube.com/watch?v=wROFvWDyugU)
> "Go !.. Go! Go!!",  Indiana Jones and the Temple of Doom, Steven Spielberg, 1984



Objectifs:
- Comprendre ce qu'est un LLM
- Activer et configurer un LLM Engine
- Utiliser l'IA G√©n√©rative pour g√©n√©rer rapidement des phrases et entra√Æner plus rapidement le mod√®le

## Sommaire

- [Introduction](#introduction)
  - [LLM et mod√®les de fondation](#llm-et-modeles-de-fondation)


- [Lancer les services gen-ai](#lancer-les-services-gen-ai)


- [Installer Ollama](#installer-ollama)
  - [R√©cup√©rer les mod√®les pour l'atelier sur la machine GPU](#r√©cup√©rer-les-mod√®les-pour-latelier-sur-la-machine-gpu)
    - [Autorisation de la registy locale insecure](#autorisation-de-la-registry-locale-insecure)
  - [R√©cup√©rer les mod√®les depuis internet hors du Codelab](#r√©cup√©rer-les-mod√®les-depuis-internet-hors-du-codelab)
  - [Tester le prompt](#tester-le-prompt)
  - [‚ö†Ô∏è Fallback : Ollama ne marche pas](#-fallback--ollama-ne-marche-pas)


- [Gen AI - Sentence generation](#gen-ai---sentence-generation)
  - [Ollama](#Ollama)
      - [configuration sous Linux](#configuration-sous-linux)
      - [Configuration sous MacOs](#configuration-sous-macos)
      - [Tester l'acc√®s √† Ollama](#tester-lacc√®s-√†-ollama)
      - [Configurer Ollama dans le generate sentence](#configurer-ollama-dans-le-generate-sentence)
  - [Configurer OpenAI dans le generate sentence](#configurer-openai-dans-le-generate-sentence)
  - [Configurer AzureOpenAI dans le generate sentence](#configure-azureopenai-dans-le-generate-sentence)


- [Ressources](#ressources)
- [√âtape suivante](#√©tape-suivante)

## Introduction

Cette introduction a pour objectif d‚Äôexpliquer la notion de LLM.

## LLM et mod√®les de fondation

Un mod√®le de langage √† grande √©chelle (LLM, pour Large Language Model) est un syst√®me d‚Äôintelligence artificielle con√ßu 
pour comprendre et g√©n√©rer du texte en s‚Äôappuyant sur un vaste corpus d‚Äôapprentissage. Entra√Æn√© sur des milliards de 
param√®tres, il peut analyser et produire du contenu textuel qui imite le langage humain avec un niveau de coh√©rence et 
de pertinence remarquable. Ces mod√®les fonctionnent gr√¢ce √† des architectures de r√©seaux de neurones profonds, 
principalement bas√©es sur des transformers, qui leur permettent de capturer les nuances linguistiques, les contextes et 
les relations s√©mantiques complexes. Les LLM peuvent accomplir diverses t√¢ches linguistiques comme r√©pondre √† des 
questions, r√©sumer des textes, traduire entre langues, r√©diger diff√©rents types de contenus, et m√™me raisonner sur des 
probl√®mes complexes, le tout sans avoir √©t√© sp√©cifiquement programm√©s pour chacune de ces t√¢ches. Leur capacit√© 
d‚Äôapprentissage par transfert leur permet d‚Äôadapter leurs connaissances g√©n√©rales √† des domaines sp√©cifiques avec un 
minimum d‚Äôexemples suppl√©mentaires.

Les mod√®les de fondation sont une cat√©gorie plus large de mod√®les d‚Äôintelligence artificielle qui servent de base pour 
de multiples applications et adaptations. Un mod√®le de fondation peut √™tre un LLM, mais il peut aussi √™tre entra√Æn√© pour 
traiter des images, de l‚Äôaudio, ou des donn√©es multimodales (combinant plusieurs types de m√©dias). Ces mod√®les sont 
con√ßus pour √™tre polyvalents et r√©utilisables dans divers contextes, pouvant √™tre adapt√©s et affin√©s pour des t√¢ches 
sp√©cifiques sans n√©cessiter un r√©-entra√Ænement complet.
Les mod√®les de fondation se caract√©risent par leur adaptabilit√© et leur capacit√© √† effectuer un large √©ventail de t√¢ches 
disparates avec pr√©cision en fonction des instructions re√ßues. Ils se divisent principalement en trois cat√©gories : 

- ceux d√©di√©s au traitement du langage naturel (comme GPT-4 ou Llama)
- ceux sp√©cialis√©s dans la vision par ordinateur (comme Stable Diffusion ou DALL-E)
- les syst√®mes multimodaux capables d‚Äôint√©grer plusieurs types de donn√©es.

Contrairement aux mod√®les traditionnels qui sont limit√©s √† une fonction sp√©cifique, les mod√®les de fondation sont 
entra√Æn√©s sur des donn√©es √† grande √©chelle et d√©veloppent une compr√©hension profonde des donn√©es initiales, ce qui leur 
permet d‚Äô√™tre ensuite affin√©s pour des cas d‚Äôusage pr√©cis.

Cette approche de pr√©entra√Ænement suivie d‚Äôajustements 
sp√©cifiques constitue leur principe fondamental de fonctionnement.

<img src="img/fondation_model.png" alt="fondation model">

Exemple non exhaustif de mod√®les de fondation


Pour utiliser une analogie de la fus√©ologie, consid√©rez les **LLM** comme des **moteurs de fus√©e** sp√©cialis√©s con√ßus pour 
propulser des missions sp√©cifiques (dans ce cas, le traitement du langage). Les **mod√®les de fondation**, en revanche, 
sont comme des **plates-formes de lancement** modulaires qui peuvent soutenir diff√©rents types de missions ‚Äî qu'il s'agisse
de lancer un satellite, d'envoyer un rover sur Mars ou de mettre en orbite un t√©lescope spatial.
<img src="img/rocket-apollo-11.png" alt="fus√©e apollo 11">

Vous en conviendrez que si la plates-forme de lancement (**Fondation Model**) est solide et droite, la fus√©e (le **LLM**) 
fait un strike dans l'espace, en revanche si le la plates-forme de lancement est bancale et que la fus√©e part chez le 
voisin, il risque d'y avoir des d√©g√¢ts !

En se basant sur des mod√®les de fondation solides et √©prouv√©s, les d√©veloppeurs peuvent cr√©er des applications qui utilisent l'IA de mani√®re plus efficace et plus s√ªre.

## Lancer les services gen-ai

Dans le fichier `docker/docker-compose.yml` d√©commenter les services `gen_ai_orchestrator-server:`, `postgres-db:` et relancer un :
```bash
cd docker
source .env
docker compose -p devoxx_tock up -d
```

Vous devriez avoir ce rendu depuis la console :

<img src="img/gen_ai_docker_compose_devoxx_2025.png" alt="gen ai docker compose">

Si vous tout est bon en vous rendant sur cette page : http://localhost:8000/docs, vous devriez avoir cette page Swagger

<img src="img/swagger_gen_ai_orchestrator.png" alt="swagger">

## Installer Ollama

Pour installer Ollama, vous devez aller sur le lien suivant : https://ollama.com/ et suivre les instructions pour t√©l√©charger Ollama sur votre machine. Une fois que cela est fait, d√©zipper le fichier et installer le programme sur votre machine. A la fin de l‚Äôinstallation Ollama vous conseil d‚Äôinstaller un model sur votre machine. Ce mod√®le fait plus de 6Go, et nous n‚Äôallons pas en avoir besoin. Il faut donc d√©cliner le t√©l√©chargement de ce mod√®le.

<img src="img/ollama.png"  alt="ollama">

### R√©cup√©rer les mod√®les pour l'atelier sur la machine GPU

Pour √©viter de congestionner le r√©seau, nous avons pr√©-t√©l√©charg√© les mod√®les pour vous et h√©berg√© une registry ollama locale √† partir de laquelle vous pouvez le mod√®le **nomic-embed-text:latest** pour l'embedding et dans un premier temps **qwen2.5:1.5b** :

| Mod√®le                      | Type            | Pull                                                                                | Recommandation RAM vid√©o min (en Q4_K_M) |
|-----------------------------|-----------------|-------------------------------------------------------------------------------------|------------------------------------------|
| **nomic-embed-text:latest** | Embedding       | `ollama pull --insecure http://gpu-server.lan:9200/library/nomic-embed-text:latest` | Pass partout üòå                          |
| **qwen2.5:1.5b**            | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/qwen2.5:1.5b`            | >2 GB (4bit quantized)                   |
| **qwen2.5:3b**              | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/qwen2.5:3b`              | >2 GB (4bit quantized)                   |
| **mistral:7b**              | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/mistral:7b`              | >4 GB (4bit quantized)                   |
| qwen2.5:7b                  | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/qwen2.5:7b`              | >4 GB (4bit quantized)                   |
| qwen2.5:14b                 | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/qwen2.5:14b`             | >8 GB (4bit quantized)                   |
| phi4:14b                    | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/phi4:14b`                | >8 GB (4bit quantized)                   |
| gemma3:4b                   | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/gemma3:4b`               | >2 GB (4bit quantized)                   |
| gemma3:12b                  | Text Generation | `ollama pull --insecure http://gpu-server.lan:9200/library/gemma3:12b`              | >7 GB (4bit quantized)                   |

Les mod√®les en gras sont les mod√®les recommand√©s pour vos tests, ne t√©l√©chargez pas tout les mod√®les üòâ, nous en avons mis plusieurs √† votre disposition pour ceux qui souhaitent tester.

```bash

ollama list # Devrait vous afficher les mod√®les
```


### R√©cup√©rer les mod√®les depuis internet hors du Codelab

Pour utiliser les mod√®les en dehors de l'environnement d'atelier, nous vous recommandons de t√©l√©charger diff√©rents 
mod√®les selon les capacit√©s de votre machine :

- **qwen2.5:1.5b** id√©al pour les machines avec des ressources limit√©es
- **Mistral** et **gemma**, recommand√©s si vous disposez d'au moins 16 Go de RAM

Pour la partie embedding (vectorisation du texte), nous utiliserons :
- **nomic-embed-text**

La r√©cup√©ration d'un mod√®le depuis Ollama s'effectue simplement via la commande suivante :

```bash
ollama pull <le_nom_de_votre_mod√®le>
```

Dans notre cas, nous allons r√©cup√©rer les mod√®les suivants en tapant ces comamndes dans le terminal :
```
ollama pull mistral
ollama pull nomic-embed-text
```

Pour v√©rifier que nous avons les mod√®les sur notre machine, il suffit de taper cette commande dans notre terminal pour avoir ce type de rendu :

```bash
 ollama list
 ```

<img src="img/ ollama_list.png" alt="ollama list">

üí°Si vous souhaitez en savoir plus sur les mod√®les, c'est par ici :

| Mod√®le            | Lien                                                                                       |
|-------------------|--------------------------------------------------------------------------------------------|
| Tinyllama         | [https://ollama.com/library/tinyllama](https://ollama.com/library/tinyllama)               |
| Gemma             | [https://ollama.com/library/gemma](https://ollama.com/library/gemma)                       |
| Mistral üá´üá∑      | [https://ollama.com/library/mistral](https://ollama.com/library/mistral)                   |
| Nomic-embed-text  | [https://ollama.com/library/nomic-embed-text](https://ollama.com/library/nomic-embed-text) |
 

### Tester le prompt

```bash
ollama run mistral
```

<img src="img/ollama_run_mistral.png" alt="ollama run mistral">

Une fois ce mod√®le t√©l√©charg√© et toujours dans le terminal, vous pouvez tester/jouer avec le mod√®le (entrez une question 
pour voir si le mod√®le r√©pond), ou quitter en appuyant sur CTRL + D.

### ‚ö†Ô∏è Fallback : Ollama ne marche pas

Installation trop lente ? √ßa rame .... pas de soucis vous allez pouvoir utiliser le serveur Ollama pr√©sent sur **http://gpu-server.lan:11434**.
N'installez pas Ollama passez √† la suite.

Modifiez dans le fichier `docker/.env` les lignes suivantes pour utiliser le serveur ollama du codelab :
```bash
# Ollama (requires RAM and works better with a GPU)
#   export OLLAMA_SERVER=host-gateway # LOCAL ollama server
#   export OLLAMA_SERVER=192.168.20.2 # OUR CODELAB ollama server at gpu-server.lan, unfortunately docker compose needs an IP addr
export OLLAMA_SERVER=192.168.20.2 # OUR CODELAB ollama server at gpu-server.lan, unfortunately docker compose needs an IP addr
```


## Gen AI - Sentence generation

D√©sormais, on va utiliser notre l'IA G√©n√©rative (notre LLM) pour g√©n√©rer rapidement des nouvelles phrases et variantes.
Cela permettra d'entra√Æner plus rapidement notre mod√®le initial, donc d'am√©liorer la compr√©hension du bot.
Pour autant, le bot utilisera toujours les m√™mes r√©ponses qu'auparavant. On ne donne donc pas "carte blanche" √† l'IA
G√©n√©rative, ce n'est pas elle qui r√©pond, elle ne peut pas improviser ou halluciner.
Pour le moment, elle am√©liore donc la compr√©hension du bot, sans perdre le contr√¥le sur les diff√©rentes r√©ponses du bot.

Le menu **Gen AI** > **Sentence Generation Settings** permet de configurer la g√©n√©ration de phrases d'entra√Ænement pour les bots FAQ.

> Remarque : pour acc√©der √† cette page, il faut b√©n√©ficier du r√¥le **_botUser_**.

![G√©n√©ration des phrases - Configuration](img/sentence-generation-settings-page.png "Ecran de configuration")

Pour activer la fonction de g√©n√©ration de phrases, vous devez choisir :

**Un provider IA :**
- Voir la [liste des fournisseurs d'IA](providers/gen-ai-provider-llm-and-embedding.md)


**Une "temp√©rature" (par d√©faut pour les nouvelles phrases) :**
- Cela correspond au degr√© d‚Äôinventivit√© du mod√®le utilis√© pour g√©n√©rer des phrases.
- La temp√©rature est situ√©e entre 0 et 1.0.
  - 0 = pas de latitude dans la cr√©ation des phrases
  - 1.0 = grande latitude dans la cr√©ation des phrases

**Un prompt :**
- C'est un ensemble d'instructions permettant de cadrer la g√©n√©ration de nouvelles phrases d'entra√Ænement.

**Le nombre de phrases :**
- Nombre de phrases d'entra√Ænement g√©n√©r√©es par chaque requ√™te.

**Activation :**
- Permet d'activer la fonctionnalit√©.


### Ollama

Si vous avez bien suivi l'[√©tape 1](step_1.md) du codelab, Ollama est install√© avec tinyOllama sur votre machine.

Avec notre environnement Docker, Ollama doit etre accessible sur le r√©seau 0.0.0.0.

### Configuration sous Linux

Si vous √™tes sur Linux, nous vous invitons √† suivre ces √©tapes.

Pour exposer ollama √† toutes les adresses IP, il faut aller modifier le fichier /etc/systemd/system/ollama.service.
Changer les lignes suivantes :    
```markdown
[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
#...
Environment="OLLAMA_HOST=0.0.0.0:11434"
```

Puis red√©marrer le service ollama avec les commandes suivantes :
```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama.service
```

### Configuration sous MacOs

Sur MacOs pour exposer Ollama sur l'ip 0.0.0.0, suivez les instructions de cette issue : https://github.com/ollama/ollama/issues/3581#issuecomment-2052338405

```bash
launchctl setenv OLLAMA_HOST "0.0.0.0"
# Si install√© via homebrew : restart ollama
brew services restart ollama 
# Sinon en mode graphique via l'icon en haut
```


### Tester l'acc√®s √† Ollama

Assurez-vous que Ollama est bien accessible sur l'ip en vous rendant sur l'url suivante : http://0.0.0.0:11434/. 
Vous devriez avoir ce rendu.

<img src="img/ollama-is-runing.png" alt="ollam is runing">

### Configurer Ollama dans le generate sentence

<details>
  <summary>Voir la configuration Ollama</summary>

Pour connecter ollama √† Tock studio, il vous faut renseigner l‚Äôacc√®s √† Ollama via cette url d‚Äôacc√®s : http://ollama-server:11434 .
Pour le mod√®le, l√† c‚Äôest √† vous de renseigner le nom du mod√®le que vous utilisez dans ce CodeLab (ici nous avons tinyllama)

<img src="img/llm-engine-ollama.png" alt="exemple de configuration avec Ollama">

</details>


### Configurer OpenAI dans le generate sentence

<details>
  <summary>Voir la configuration openAI</summary>

Si vous souhaitez utiliser openAI, vous devez vous inscrire sur la plateforme [OpenAI](https://platform.openai.com/docs/introduction)
pour obtenir une cl√© d'API (attention cela n√©cessite d'acheter des cr√©dits).
Une fois cela fait rendez-vous √† cette page [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys) 
pour g√©n√©rer votre cl√© d'API.

D√®s que vous avez votre cl√© d'API, vous pouvez la renseigner dans le champ **API Key** et choisir le model (**Model name**) que vous souhaitez utiliser.
Par exemple vous pourriez avoir ce genre de rendu.

<img src="img/llm-engine-openai.png" alt="exemple de configuration avec openAI">

</details>


### Configure AzureOpenAI dans le generate sentence

<details>
  <summary>Voir la configuration Azure</summary>

Si vous souhaitez utiliser Azure OpenAI, vous devez vous inscrire sur la plateforme
[Azure OpenAI](https://azure.microsoft.com/fr-fr/products/ai-services/openai-service) et d'avoir un compte professionnel  
afin d'avoir une cl√© d'API.
Une fois cela fait, vous pouvez renseigner votre cl√© d'API dans le champ **API Key** et choisir le model (**Model name**)
que vous souhaitez utiliser.


<img src="img/gen-ai-settings-sentence-generation.png" alt="exemple de configuration avec Azure OpenAI">

</details>

## G√©n√©rer des phrases d'entra√Ænement
Pour v√©rifier que l'IA g√©n√©rative est bien configur√©e, allez dans **Stories & Answers** > **FAQs stories**. 
L√†, vous allez cliquer sur **+ NEW FAQ STORY**.

<img src="img/new-faq-with-IA.png" alt="new faq story">

Dans l‚Äôonglet **QUESTION** et dans le champ comportant le m√™me champ.
Pour l‚Äôexemple, nous avons cette phrase ¬´ bonjour le bot ¬ª que nous ajoutons comme question en appuyant sur le **+**.

<img src="img/add-question-faq-stories-with-ai.png" alt="add question">

D√®s que cela est fait, cliquez sur l‚Äôic√¥ne **Generate sentences**.

<img src="img/generate-sentence-with-ai.png" alt="Generate sentence">

Cela va ouvrir une pop-up comme celle-ci vous permettant de g√©n√©rer des mots ou des phrases.

<img src="img/pop-up-generate-sentence.png" alt="generate sentence">

L√†, vous allez choisir votre phrase que vous avez renseign√© juste avant puis choisir les √©l√©ments de langages que vous 
souhaitez g√©n√©rer. Une fois cela fait, cliquer sur **GENERATE**

<img src="img/pop-up-generate-sentence-last-step.png" alt="generate sentence">

Apr√©s quelques secondes vous devriez avoir ce genre de rendu.

<img src="img/result-generat-sentence-ai.png" alt="resultat gen ai">

Vous pouvez tout s√©lectionner puis valider, chose qui vous ram√®nera √† la page de la FAQ. 
L√†, vous pourrez voir que les questions g√©n√©rer par l‚ÄôIA ont √©t√© import√©es.

<img src="img/import-gen-ai-sentence.png" alt="import sentence">

Vous pouvez cliquer ensuite sur l‚Äôonglet **Answer** pour r√©diger une r√©ponse, puis cliquer sur **SAVE**.


## Ressources
| Titre                                                                                  | Lien                                                                                                                                                                                                                                             |
|----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Ce qui se cache derri√®re le fonctionnement de ChatGPT (ScienceEtonnante)               | [https://www.youtube.com/watch?v=7ell8KEbhJo](https://www.youtube.com/watch?v=7ell8KEbhJo)                                                                                                                                                                             |
| Comprendre et utiliser les mod√®les de langage d'IA (S√©bastien COLLET @ Devoxx 2023)    | [https://www.youtube.com/watch?v=ZbWL2W53BXY](https://www.youtube.com/watch?v=ZbWL2W53BXY)                                                                                                                                                                             |
| Attention Is All You Need                                                              | [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)                                                                                                                                                                             |
| The Illustrated Transformer                                                            | [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)                                                                                                                                         |
| Evaluating Large Language Model (LLM) systems: Metrics, challenges, and best practices | [https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5](https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5) |
| Le Prompt Engineering : L'art de converser avec l'intelligence artificielle            | [https://blog.lesjeudis.com/le-prompt-engineering](https://blog.lesjeudis.com/le-prompt-engineering)                                                                                                                                             |
| Influence response generation with inference parameters                                | [https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html)                                                                                 |
| Demystifying AI Inference Deployments for Trillion Parameter Large Language Models     | [https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/](https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/)   |
| An Evaluation of Vector Database Systems: Features, and Use Cases                      | [https://blog.devgenius.io/an-evaluation-of-vector-database-systems-features-and-use-cases-9a90b05eb51f](https://blog.devgenius.io/an-evaluation-of-vector-database-systems-features-and-use-cases-9a90b05eb51f)   | 
| Awesome Foundation Models                                                              | [https://github.com/uncbiag/Awesome-Foundation-Models?tab=readme-ov-file](https://github.com/uncbiag/Awesome-Foundation-Models?tab=readme-ov-file)                                                                                               | 
| Que sont les mod√®les de fondation ?                                                    | [https://aws.amazon.com/what-is/foundation-models/](https://aws.amazon.com/what-is/foundation-models/)                                                                                                       |


## √âtape suivante

- [√âtape 4](step_4.md)

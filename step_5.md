# Activer l'IAGen dans notre bot

[<img src="img/Indiana_Jones_and_the_Last_Crusade_Motorcycle_Chase.png"  alt="Indiana Jones and the Last Crusade">](https://www.youtube.com/watch?v=e1KKVy-nki8)

> "The solution presents itself !", Indiana Jones Last Crusade, Steven Spielberg, 1989

<br/>
<u>Objectifs:</u>

- Configurer les param√®tres de l'IA g√©n√©rative
- Activer l'IA g√©n√©rative dans Tock Studio
- Interroger le bot


## Sommaire

<!-- TOC -->
* [Activer l'IAGen dans notre bot](#activer-liagen-dans-notre-bot)
  * [Sommaire](#sommaire)
  * [La chaine RAG üîó (Condensation, Retrieval, Generation)](#la-chaine-rag--condensation-retrieval-generation)
  * [Configuration](#configuration)
    * [Condensation](#condensation)
    * [G√©n√©ration](#g√©n√©ration)
    * [Mod√®le de vectorisation - Embedding](#mod√®le-de-vectorisation---embedding)
      * [Configurer Ollama pour Emdedding](#configurer-ollama-pour-emdedding)
      * [Configurer OpenAi pour Emdedding](#configurer-openai-pour-emdedding)
      * [Configurer AzureOpenAI pour Emdedding](#configurer-azureopenai-pour-emdedding)
    * [Autres exemples de configurations LLM (condensation / generation)](#autres-exemples-de-configurations-llm-condensation--generation)
    * [Configurer OpenAi pour LLM Engine](#configurer-openai-pour-llm-engine)
    * [Configurer AzureOpenAI pour LLM Engine](#configurer-azureopenai-pour-llm-engine)
    * [Configuration final et activation](#configuration-final-et-activation)
  * [Interroger le bot](#interroger-le-bot)
  * [√âtape suivante](#√©tape-suivante)
<!-- TOC -->



## La chaine RAG üîó (Condensation, Retrieval, Generation)

Avant de configurer notre RAG il est important de noter que TOCK propose par d√©faut une chaine RAG avec un maillon suppl√©mentaire.

```mermaid
flowchart TD
  start["`**Requ√™te utilisateur**
  
  ü§ñ : Umbrella Academy est une s√©rie ...
  üë® : Qui est son auteur ?
  `"]
  condensationStage["`**Condensation**
  *Contextualisation de la question*
  
  Prompt - **Question Condensing** :
  Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be nderstood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.
  
  Chat history : [ü§ñ : Umb..., üë® : Qui est son auteur ?]
  
  Condensed question :
  `"]
  retrievalStage["`**Retrieval**
  *Recherche des documents en base vectorielle avec la question contextualis√©e*
  
  'Qui est l'auteur d'Umbrella Academy' -- vectoris√© / mod√®le embedding --> [0.2311, 0.1116, ...]
  Ce vecteur permet de faire la recherche en base.
  `"]
  augmentedGeneration["`**Generation**
  *G√©n√®re la r√©ponse avec la question et documents*
  
  Prompt - **Question Answering** :
  Your are Devoxxy a TV show robot ....
  Using the following context [üóÇÔ∏è Fiche s√©rie Umbrella Academy].
  Answer the question : Qui est l'auteur d'Umbrella Academy
  `"]
  final["ü§ñ : Umbrella Academy a √©t√© cr√©√© par  Steve Blackman. "]
  
  start -- üí¨ Qui est son auteur ? --> condensationStage
  condensationStage -- üí¨ Qui est l'auteur <br> d'Umbrella Academy --> retrievalStage
  retrievalStage -- üóÇÔ∏è Fiche s√©rie Umbrella Academy --> augmentedGeneration
  augmentedGeneration --> final
```

Nous allons maintenant voir comment configurer ces diff√©rentes maillons de la chaine.
Tock vous permet de personnaliser les prompts et d√©finir les mod√®les utilis√©s.

## Configuration

Dans le menu de gauche au niveau **Gen AI** > **RAG Settings** (Retrieving augmented Generation) vous allez pouvoir 
choisir les mod√®les d'IA g√©n√©rative pris en charge par Tock et de configurer un ensemble de crit√®res sp√©cifiques √† chaque fournisseur d'IA.
Cette fonctionnalit√© permettra √† TOCK de g√©n√©rer une r√©ponse √† une requ√™te de l'utilisateur, sur la base d'un ensemble de documents int√©gr√©s dans une base de donn√©es vectorielle.

> Pour acc√©der √† cette page il faut b√©n√©ficier du r√¥le **_botUser_**, role que vous avez en √©tant admin.

### Condensation

Il s'agit du mod√®le utilise pour contextualiser la question de l'utilisateur avec l'historique de la conversation.

√âcran d'exemple d'une configuration ollama avec le mod√®le `qwen2.5:3b`, pull depuis la machine gpu.
Vous pouvez tr√®s bien utiliser un autre mod√®le.

![RAG - Condensation LLM](img/gen-ai-setting-rag-condensing-ollama.png)

**Un provider IA :** (LLM Engine)

<img src="img/llm-engine.png" alt="llm engine">

- Cette section permet de param√©trer les options li√©es au mod√®le IA qui g√©n√®re la r√©ponse √† l‚Äôutilisateur.
- Voir la [liste des fournisseurs d'IA](providers/gen-ai-provider-llm-and-embedding.md)


**Temp√©rature :**

<img src="img/temperature.png" alt="temperature">

- On peut d√©finir une temp√©rature situ√©e entre 0 et 1.
- Celle-ci permet de d√©terminer le niveau de cr√©ativit√© du Bot dans la r√©ponse apport√©e √† une requ√™te qui lui est envoy√©e.


### G√©n√©ration

Il s'agit du LLM utilis√© pour g√©n√©ration la r√©ponse finale avec les √©l√©ments de la base documentaire.

√âcran d'exemple d'une configuration ollama avec le mod√®le `qwen2.5:3b`, pull depuis la machine gpu.
Vous pouvez tr√®s bien utiliser un autre mod√®le.

![RAG - Generation LLM](img/gen-ai-settings-rag-question-answering-llm-setting-ollama.png)

*Voir section pr√©c√©dent pour les explications concernant la temp√©rature et le fournisseur IA.*

**Prompt :**

<img src="img/prompt.png" alt="prompt">

- Le prompt est le script qui d√©termine la personnalit√© du Bot, le contexte dans lequel il doit r√©pondre, la fa√ßon dont il doit s‚Äôadresser √† l‚Äôutilisateur, les recommandations ou instructions de r√©ponses, les styles et formats de r√©ponses.

Nous personnaliserons le prompt dans une prochaine √©tape üòâ.

### Mod√®le de vectorisation - Embedding

Ce mod√®le est utilis√© pour g√©n√©rer des vecteurs repr√©sentant s√©mantiquement le sens des la question utilisateur il doit 
√™tre le m√™me que celui utilis√© pendant la phase d'ingestion.

Nous vous conseillons de suivre la configuration Ollama.

#### Configurer Ollama pour Emdedding

<details>
  <summary>Voir la configuration Ollama</summary>

Pour connecter Ollama √† Tock studio sur la partie embedding, il vous faut renseigner l‚Äôacc√®s √† Ollama via cette url d‚Äôacc√®s (**BaseUrl**) : http://ollama-server:11434.
Pour le mod√®le (**Model**), l√† c‚Äôest √† vous de renseigner le nom du mod√®le que vous utilisez dans ce CodeLab
(ici nous avons **nomic-embed-text**).

‚ö†Ô∏è Si Ollama est lanc√© en local depuis votre ordinateur vous devez changer la valeur de base url de d√©faut par "http://host.docker.internal:11434"

<img src="img/gen-ai-settings-rag-embedding-ollama.png" alt="embedding ollama">

Pour le reste de configuration, nous vous invitons √† aller directement au chapitre [Configuration final et activation](#configuration-final-et-activation)
</details>



#### Configurer OpenAi pour Emdedding
<details>
  <summary>Voir la configuration OpenAi</summary>

Si vous souhaitez utiliser openAI, vous devez vous inscrire sur la plateforme [OpenAI](https://platform.openai.com/docs/introduction)
pour obtenir une cl√© d'API. Une fois cela fait rendez-vous √† cette page [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
pour g√©n√©rer votre cl√© d'API.

D√®s que vous avez votre cl√© d'API, vous pouvez la renseigner dans le champ **API Key** et choisir le model
(**Model name**) actuellement disponible : **text-embedding-ada-002**.
Par exemple vous pourriez avoir ce genre de rendu.

<img src="img/embedding-openai-settings.png" alt="embedding openai">

Pour le reste de configuration, nous vous invitons √† aller directement au chapitre [Configuration final et activation](#configuration-final-et-activation)
</details>

#### Configurer AzureOpenAI pour Emdedding

<details>
  <summary>Voir la configuration AzureOpenAI</summary>

Si vous souhaitez utiliser Azure OpenAI, vous devez vous inscrire sur la plateforme
[Azure OpenAI](https://azure.microsoft.com/fr-fr/products/ai-services/openai-service) et d'avoir un compte professionnel  
afin d'avoir une cl√© d'API.
Une fois cela fait, vous pouvez renseigner votre cl√© d'API dans le champ **API Key** et choisir le model (**Model name**)
que vous souhaitez utiliser.

<img src="img/embedding-azureopenai-settings.png" alt="embedding azure">

Pour le reste de configuration, nous vous invitons √† aller directement au chapitre [Configuration final et activation](#configuration-final-et-activation)
</details>


### Autres exemples de configurations LLM (condensation / generation)

Si vous ne souhaitez pas utiliser Ollama comme LLM de condensation et / ou g√©n√©ration.
Voici des exemples avec OpenAI et Azure OpenAI.

### Configurer OpenAi pour LLM Engine
<details>
  <summary>Voir la configuration OpenAi</summary>

Si vous souhaitez utiliser openAI, vous devez vous inscrire sur la plateforme [OpenAI](https://platform.openai.com/docs/introduction)
pour obtenir une cl√© d'API. Une fois cela fait rendez-vous √† cette page [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys) pour g√©n√©rer votre cl√© d'API.

D√®s que vous avez votre cl√© d'API, vous pouvez la renseigner dans le champ **API Key** et choisir le model (**Model name**) que vous souhaitez utiliser.
Par exemple vous pourriez avoir ce genre de rendu.

<img src="img/rag-settings-example-openai.png" alt="rag settings  openai">
</details>


### Configurer AzureOpenAI pour LLM Engine
<details>
  <summary>Voir la configuration AzureOpenAI</summary>

Si vous souhaitez utiliser Azure OpenAI, vous devez vous inscrire sur la plateforme
[Azure OpenAI](https://azure.microsoft.com/fr-fr/products/ai-services/openai-service) et d'avoir un compte professionnel  
afin d'avoir une cl√© d'API.
Une fois cela fait, vous pouvez renseigner votre cl√© d'API dans le champ **API Key** et choisir le model (**Model name**)
que vous souhaitez utiliser.

<img src="img/rag-settings-example-azure.png" alt="rag settings azure">
</details>


### Configuration final et activation
Cette partie s‚Äôattarde sur les intitul√©s **Indexing session** et **Conversation flow**.

<img src="img/indexing-and-conversation-flow.png" alt="indexing and conversation flow">

> **Note importante :** 
>
> Il faut que l‚Äôingestion de donn√©es et cette configuration finale utilise le m√™me LLM

En [√âtape 4](step_4.md), vous avez r√©alis√© une ingestion de donn√©es, via un programme en python.
Normalement, si l‚Äôingestion a √©t√© correctement r√©alis√© le programme a termin√© avec un succ√®s qui affiche des Id, comme dans l‚Äôexemple suivant :

<img src="img/python-ingestion-result.png" alt="r√©sultat de l'ingestion en python">

<details>
  <summary> üôã‚Äç‚ôÇÔ∏èJe suis sur l'instance partag√© je n'ai pas mon ID de session d'indexation</summary>

 Vous trouverez l'ID de sessions d'ingestion dans le [Indiana Jones Tock Studio Accounts - Lab Server - Devoxx2025 - Google Sheet](https://docs.google.com/spreadsheets/d/1oNl4oBIJ0TEyhGZRk19Jzi8NHMLcialaBVY95lk-jq4/edit?usp=sharing).
</details>

Dans ce r√©sultat, vous avez une variable nomm√©e **Index session ID** qui fournit un identifiant unique.
Copier cet identifiant pour le coller dans le champ **Indexing session id**.
Vous pouvez √©galement choisir le nombre max de documents retourn√© par la recherche en base documentaire et utilis√©s pour
g√©n√©rer la r√©ponse, limiter ce nombre est int√©ressant pour r√©duire la consommation de token.

Enfin l'option "Don't allow undocumented answers" si elle est activ√©e emp√™che la g√©n√©ration de r√©ponse si aucun document n'est trouv√©,
certains utilisateurs pr√©f√®re avoir quand m√™me une r√©ponse par exemple le LLM qui demande √† pr√©ciser la question ... dans ce cas il vaut mieux le d√©sactiver.

Dans l‚Äôintitul√© **Conversation flow** et dans le champ **No answer sentence**, vous devez sp√©cifier une phrase lorsque l‚ÄôIA n‚Äôest pas capable de r√©pondre.
Par la m√™me occasion, vous pouvez aussi rediriger vers une story sp√©cifique dans le champ **Unanswered story redirection**

Avant de sauvegarder tout √ßa penser √† activer le Rag et les donn√©es de debug, √ßa nous aidera par la suite en haut de la page :
![Enable RAG and debug](img/gen-ai-rag-enable-and-debug.png)

Par exemple si on met √† jour les diff√©rents √©l√©ments et que vous sauvegardez en cliquant en haut sur le bouton bleu 
**SAVE**, vous devriez voir apparaitre une banni√®re verte signifiant que tout est correcte.

<img src="img/rag-settings-success.png" alt="rag settings final">

> Il est possible de sauvegarder les param√®tre sans activer le RAG dans ce cas aucun check n'est fait.
> Sinon un appel d'embedding et au divers LLM et base vectorielle est fait.

## Interroger le bot

Maintenant vous pouvez discuter avec le bot et lui poser des questions.

<img src="img/rag-question.png" alt="question avec le rag">

## √âtape suivante

- [√âtape 6](step_6.md)
